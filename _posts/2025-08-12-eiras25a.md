---
title: 'Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges'
abstract: 'Large Language Model (LLM) based judges form the underpinnings of key safety
  evaluation processes such as offline benchmarking, automated red-teaming, and online
  guardrailing. This widespread requirement raises the crucial question: can we trust
  the evaluations of these evaluators? In this paper, we highlight two critical challenges
  that are typically overlooked: (i) evaluations in the wild where factors like prompt
  sensitivity and distribution shifts can affect performance and (ii) adversarial
  attacks that target the judge. We highlight the importance of these through a study
  of commonly used safety judges, showing that small changes such as the style of
  the model output can lead to jumps of up to 0.24 in the false negative rate on the
  same dataset, whereas adversarial attacks on the model generation can fool some
  judges into misclassifying 100% of harmful generations as safe ones. These findings
  reveal gaps in commonly used meta-evaluation benchmarks and weaknesses in the robustness
  of current LLM judges, indicating that low attack success under certain judges could
  create a false sense of security.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: eiras25a
month: 0
tex_title: 'Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges'
firstpage: 56
lastpage: 66
page: 56-66
order: 56
cycles: false
bibtex_author: Eiras, Francisco and Zemour, Eliott and Lin, Eric and Mugunthan, Vaikkunth
author:
- given: Francisco
  family: Eiras
- given: Eliott
  family: Zemour
- given: Eric
  family: Lin
- given: Vaikkunth
  family: Mugunthan
date: 2025-08-12
address:
container-title: 'Proceedings on "I Can''t Believe It''s Not Better: Challenges in
  Applied Deep Learning" at ICLR 2025 Workshops'
volume: '296'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 8
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v296/main/assets/eiras25a/eiras25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
