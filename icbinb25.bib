@proceedings{icbinb-2023,
  booktitle =	 {Proceedings on "I Can't Believe It's Not Better: Challenges
                in Applied Deep Learning" at ICLR 2025 Workshops},
  editor =	 {Blaas, Arno and D'Costa, Priya and 
              Feng, Fan and Kriegler, Andreas and Mason, Ian and 
              Pan, Zhaoying and Uelwer, Tobias and 
              Williams, Jennifer and Xie, Yubin and Yang, Rui},
  year =	 2025,
  shortname =	 {ICBINB 25},
  volume =	 296,
  start =	 {2025-04-28},
  end =		 {2025-04-28},
  published =	 {2025-08-12},
  address =	 {Singapore},
  conference_url ={https://sites.google.com/view/icbinb-2025/},
  conference_number =5,
}

@InProceedings{toner25,
  title =	 {Performance of Zero-Shot Time Series Foundation Models on Cloud Data},
  author=  {Toner, William and Lee, Thomas L. and Joosen, Artjom and Singh, Rajkarn and Asenov, Martin},
  pages =	 {1-12},
  abstract =	 {Time series foundation models (FMs) have emerged as a popular paradigm for zero-shot multi-domain forecasting. FMs are trained on numerous diverse datasets and claim to be effective forecasters across multiple different time series domains, including cloud data. In this work we investigate this claim, exploring the effectiveness of FMs on cloud data. We demonstrate that many well-known FMs fail to generate meaningful or accurate zero-shot forecasts in this setting. We support this claim empirically, showing that FMs are outperformed consistently by simple linear baselines. We also illustrate a number of interesting pathologies, including instances where FMs suddenly output seemingly erratic, random-looking forecasts. Our results suggest a widespread failure of FMs to model cloud data.},
}

@InProceedings{rahman25,
  title =	 {Rethinking Evaluation for Temporal Link Prediction through Counterfactual Analysis},
  author=  {Rahman, Aniq Ur and Modell, Alexander and Coon, Justin}, 
  pages =	 {13-19},
  abstract =	 {In response to critiques of existing evaluation methods for temporal link prediction (TLP) models, we propose a novel approach to verify if these models truly capture temporal patterns in the data. Our method involves a sanity check formulated as a counterfactual question: ``What if a TLP model is tested on a temporally distorted version of the data instead of the real data?'' Ideally, a TLP model that effectively learns temporal patterns should perform worse on temporally distorted data compared to real data. We analyse this hypothesis and introduce two temporal distortion techniques to assess six well-known TLP models.},
}

@InProceedings{wu25,
  title =	 {Filter bubbles and affective polarization in user-personalized large language model outputs},
  author = {Wu, Han and Rowlands, Sareh and Wahlstrom, Johan},
  pages =	 {20-25},
  abstract =	 {As cloud computing becomes pervasive, deep learning models are deployed on cloud servers and then provided as APIs to end users. However, black-box adversarial attacks can fool image classification models without access to model structure and weights. Recent studies have reported attack success rates of over 95\% with fewer than 1,000 queries. Then the question arises: whether black-box attacks have become a real threat against cloud APIs? To shed some light on this, our research indicates that black-box attacks are not as effective against cloud APIs as proposed in research papers due to several common mistakes that overestimate the efficiency of black-box attacks. To avoid similar mistakes, we conduct black-box attacks directly on cloud APIs rather than local models.},
}

@InProceedings{mitra25,
  title =	 {Modeling speech emotion with label variance and analyzing performance across speakers and unseen acoustic conditions},
  author = {Mitra, Vikramjit and Romana, Amrit and Tran, Dung, and Azemi, Erdrin},
  pages =	 {26-36},
  abstract =	 {Spontaneous speech emotion data usually contain perceptual grades where graders assign emotion score after listening to the speech files. Such perceptual grades introduce uncertainty in labels due to grader opinion variation. Grader variation is addressed by using consensus grades as groundtruth, where the emotion with the highest vote is selected. Consensus grades fail to consider ambiguous instances where a speech sample may contain multiple emotions, as captured through grader opinion uncertainty. We demonstrate that using the probability density function of the emotion grades as targets instead of the commonly used consensus grades, provide better performance on benchmark evaluation sets compared to results reported in the literature. We show that a saliency driven foundation model (FM) representation selection helps to train a state-of-the-art speech emotion model for both dimensional and categorical emotion recognition. Comparing representations obtained from different FMs, we observed that focusing on overall test-set performance can be deceiving, as it fails to reveal the models generalization capacity across speakers and gender. We demonstrate that performance evaluation across multiple test-sets and performance analysis across gender and speakers are useful in assessing usefulness of emotion models. Finally, we demonstrate that label uncertainty and data-skew pose a challenge to model evaluation, where instead of using the best hypothesis, it is useful to consider the 2- or 3-best hypotheses.},
}

@InProceedings{cornell25,
  title =	 {On the Power of Heuristics in Temporal Graphs},
  author = {Cornell, Filip and Smirnov, Oleg, and Gandler, Gabriela Zarzar and Cao, Lele},
  pages =	 {37-46},
  abstract =	 {Dynamic graph datasets often exhibit strong temporal patterns, such as recency, which prioritizes recent interactions, and popularity, which favors frequently occurring nodes. We demonstrate that simple heuristics leveraging only these patterns can perform on par or outperform state-of-the-art neural network models under standard evaluation protocols. To further explore these dynamics, we introduce metrics that quantify the impact of recency and popularity across datasets. Our experiments on BenchTemp and the Temporal Graph Benchmark show that our approaches achieve state-of-the-art performance across all datasets in the latter and secure top ranks on multiple datasets in the former. These results emphasize the importance of refined evaluation schemes to enable fair comparisons and promote the development of more robust temporal graph models. Additionally, they reveal that current deep learning methods often struggle to capture the key patterns underlying predictions in real-world temporal graphs. For reproducibility, we have made our code publicly available.},
}

@InProceedings{rangel25,
  title =	 {On the Limits of Applying Graph Transformers for Brain Connectome Classification},
  author = {Rangel, Jose Miguel Lara and Heinbaugh, Clare Elizabeth},
  pages =	 {47-55},
  abstract = {Brain connectomes offer detailed maps of neural connections within the brain. Recent studies have proposed novel connectome graph datasets and attempted to improve connectome classification by using graph deep learning. With recent advances demonstrating transformers’ ability to model intricate relationships and outperform in various domains, this work explores their performance on the novel NeuroGraph benchmark datasets and synthetic variants derived from probabilistically removing edges to simulate noisy data. Our findings suggest that graph transformers offer no major advantage over traditional GNNs on this dataset. Furthermore, both traditional and transformer GNN models maintain accuracy even with all edges removed, suggesting that the dataset’s graph structures may not significantly impact predictions. We propose further assessing NeuroGraph as a brain connectome benchmark, emphasizing the need for well-curated datasets and improved preprocessing strategies to obtain meaningful edge connections.},
}


@InProceedings{eiras25,
  title =	 {Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges},
  author=  {Eiras, Francisco and Zemour, Eliott and Lin, Eric and Mugunthan, Vaikkunth},
  pages =	 {56-66},
  abstract =	 {Large Language Model (LLM) based judges form the underpinnings of key safety evaluation processes such as offline benchmarking, automated red-teaming, and online guardrailing. This widespread requirement raises the crucial question: can we trust the evaluations of these evaluators? In this paper, we highlight two critical challenges that are typically overlooked: (i) evaluations in the wild where factors like prompt sensitivity and distribution shifts can affect performance and (ii) adversarial attacks that target the judge. We highlight the importance of these through a study of commonly used safety judges, showing that small changes such as the style of the model output can lead to jumps of up to 0.24 in the false negative rate on the same dataset, whereas adversarial attacks on the model generation can fool some judges into misclassifying 100\% of harmful generations as safe ones. These findings reveal gaps in commonly used meta-evaluation benchmarks and weaknesses in the robustness of current LLM judges, indicating that low attack success under certain judges could create a false sense of security.},
}

@InProceedings{ong25,
  title =	 {Impact of Task Phrasing on Presumptions in Large Language Models},
  author =	 {Ong, Kenneth J. K.},
  pages =	 {67-74},
  abstract =	 {Concerns with the safety and reliability of applying large-language models (LLMs) in unpredictable real-world applications motivate this study, which examines how task phrasing can lead to presumptions in LLMs, making it difficult for them to adapt when the task deviates from these assumptions. We investigated the impact of these presumptions on the performance of LLMs using the iterated prisoner's dilemma as a case study. Our experiments reveal that LLMs are susceptible to presumptions when making decisions even with reasoning steps. However, when the task phrasing was neutral, the models demonstrated logical reasoning without much presumptions. These findings highlight the importance of proper task phrasing to reduce the risk of presumptions in LLMs.},
}

@InProceedings{villecroze25,
  title =	 {Last Layer Empirical Bayes},
  author =	 {Villecroze, Valentin and Wang, Yixin and Loaiza-Ganem, Gabriel},
  pages =	 {75-83},
  abstract =	 {The task of quantifying the inherent uncertainty associated with neural network predictions is a key challenge in artificial intelligence. Bayesian neural networks (BNNs) and deep ensembles are among the most prominent approaches to tackle this task. Both approaches produce predictions by computing an expectation of neural network outputs over some distribution on the corresponding weights; this distribution is given by the posterior in the case of BNNs, and by a mixture of point masses for ensembles. Inspired by recent work showing that the distribution used by ensembles can be understood as a posterior corresponding to a learned data-dependent prior, we propose last layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a normalizing flow, which is then trained to maximize the evidence lower bound; to retain tractability we use the flow only on the last layer. We show why LLEB is well motivated, and how it interpolates between standard BNNs and ensembles in terms of the strength of the prior that they use. LLEB performs on par with existing approaches, highlighting that empirical Bayes is a promising direction for future research in uncertainty quantification.},
}



@InProceedings{oladipupo25,
  title =	 {How Effective Are AI Models in Translating English Scientific Texts to Nigerian Pidgin: A Low-resource Language?},
  author =	 {Oladipupo, Flora and Soronnadi, Anthony and Adebara, Ife and Adekanmbi, Olubayo},
  pages =	 {84-89},
  abstract =	 {This research explores the challenges and limitations of applying deep learning models to the translation of scientific texts from English to Nigerian Pidgin, a widely spoken but low-resource language in West Africa. Despite advancements in machine translation, translating domain-specific content such as biological research papers presents unique obstacles, including data scarcity, linguistic complexity, and model generalization issues. We investigate the performance of AI models, including Pidgin-UNMT, mt5-base model, AfriTeVa base, Afri-mt5 base model and GPT 4.0 model through a comparative analysis using BLEU scores, CHRF, TER, Africomet metrics on a newly created Eng-PidginBioData dataset of biological texts. Our findings reveal significant gaps in model performance, emphasizing the need for more domain-specific fine-tuning, improved dataset creation, and collaboration with native speakers to enhance translation accuracy. By presenting real-world challenges encountered in applying deep learning to low-resource languages this research suggests strategies to overcome these barriers. Our study provides valuable insights into the persistent challenges faced by AI-driven translation systems, from limited data to domain mismatches, and highlights ways to enhance their effectiveness for underrepresented languages. By addressing these constraints, we offer actionable strategies for more inclusive and impactful scientific knowledge dissemination.},
}

@InProceedings{mayilvahanan25,
  title =	 {In Search of Forgotten Domain Generalization},
  author =	 {Mayilvahanan, Prasanna and Zimmermann, Roland S. and Wiedemer, Thaddäus and Rusak, Evgenia and Juhos, Attila and Bethge, Matthias and Brendel, Wieland},
  pages =	 {90-130},
  abstract =	 {Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION---LAION-Natural and LAION-Rendition---that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale---a crucial prerequisite for improving model robustness.},
}

@InProceedings{gorantla25,
  title =	 {Challenges of Decomposing Tools in Surgical Scenes Through Disentangling The Latent Representations},
  author =	 {Gorantla, Sai Lokesh and Sista, Raviteja and Srivastava, Apoorva and De, Utpal and Chakrabarti, Partha Pratim and Sheet, Debdoot},
  pages =	 {130-140},
  abstract =	 {Image generation through disentangling object representations is a critical area of research with significant potential. Disentanglement involves separating the representation of objects and their attributes, enabling greater control over the generated output. However, existing approaches are limited to disentangling only the objects’ attributes and generating images with selected combinations of attributes. This study explores learning object-level disentanglement of semantically rich latent representation using von-Mises-Fisher (vMF) distributions. The proposed approach aims to disentangle compressed representations into object and background classes. The approach is tested on surgical scenes for disentanglement of tools and background information using the Cholec80 dataset. Achieving tool-background disentanglement provides an opportunity to generate rare and custom surgical scenes. However, the proposed method learns to disentangle representations based on pixel intensities. This study uncovers the challenges and shortfalls in achieving object-level disentanglement of the compressed representations using vMF distributions. The code for this study is available at https://github.com/it-is-lokesh/vMF-disentanglement-challenges.},
}

@InProceedings{sbicego25,
  title =	 {On the Role of Structure in Hierarchical Graph Neural Networks},
  author = {Sbicego, Luca and Öğüt, Sevda and Madeira, Manuel and QIN, Yiming and Thanou, Dorina and Frossard, Pascal},
  pages =	 {141-150},
  abstract = {Hierarchical Graph Neural Networks (GNNs) integrate pooling layers to generate graph representations by progressively coarsening graphs. These GNNs are provably more expressive than traditional GNNs that solely rely on message passing. While prior work shows that hierarchical architectures do not exhibit empirical performance gains, these findings are based on small datasets where structure-unaware baselines often perform well, limiting their generalizability. In this work, we comprehensively investigate the role of graph structure in pooling-based GNNs. Our analysis includes: (1) reproducing previous studies on larger, more diverse datasets, (2) assessing the robustness of different architectures to structural perturbations of the graphs at varying depths of the network layers, and (3) comparing against structure-agnostic baselines. Our results confirm previous findings and demonstrate that they hold across newly tested datasets, even when graph structure is meaningful for the task. Interestingly, we observe that hierarchical GNNs exhibit improved performance recovery to structural perturbations compared to their flat counterparts. These findings highlight both the potential and limitations of pooling-based GNNs, motivating the need for more structure-sensitive benchmarks and evaluation frameworks.},
}

@InProceedings{kim25,
  title =	 {An Integrated YOLO and VLM System for Fire Detection in Enclosed Environments},
  author=  {Kim, Jongeun and Lee, Yejin and Yoon, Dongsik and Jung, Chansung and Lee, Gunhee},
  pages =	 {151-162},
  abstract =	 {While YOLO models show promise in car fire detection, they remain insufficient for real-world deployment in confined parking environments due to dataset limitations, evaluation gaps, and deployment constraints. We first fine-tune YOLO on a fire/smoke-augmented dataset, but analysis reveals its struggles with ambiguous fire-smoke boundaries, leading to false predictions. To address this, we propose a real-time end-to-end framework integrating YOLOv8s with Florence2 VLM, combining object detection with contextual reasoning. While YOLOv8s with VLM improves detection reliability, challenges are still ongoing. Our findings highlight YOLO’s limitations in fire detection and the need for a more adaptive, environment-aware approach.},
}